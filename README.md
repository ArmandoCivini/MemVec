<div align="center">

# MemVec

Multi-tier vector database (cache + index + S3) — minimal proof of concept.

</div>



MemVec is a simple, minimal, and opinionated proof-of-concept for a multi-tiered vector database. The core idea is to split responsibilities across three layers:

- Hot memory: a small, fast cache for frequently used vector chunks (Redis/FakeRedis)
- Fast approximate search: an in-memory ANN index (FAISS HNSW) over vector IDs
- Durable cold storage: Amazon S3 for all vectors and metadata, stored in grouped chunks

Queries hit the index to get candidate IDs, pull the required chunks from the cache, and fall back to S3 on misses. Misses are cached with a TTL so future queries stay fast.

This is a POC, not production. It’s intentionally minimal and can be optimized a lot.

## Why multi-tier memory for vector DBs?

- Latency: keep hot data in RAM for sub-100ms responses.
- Cost: store the full corpus cheaply in S3; pay RAM only for what’s hot.
- Scale: decouple index from raw vectors; scale each layer independently.

## Architecture (how MemVec works)

1. Encode: text → embedding via a pluggable embedding generator.
2. Candidate search: FAISS HNSW index returns vector IDs for top-k neighbors.
3. Grouping: IDs are grouped by their chunk_id for efficient retrieval.
4. Cache first: for each chunk_id, MemVec does a batch GET against Redis.
5. S3 fallback: missing chunks are downloaded from S3 (batch), then inserted into cache with a TTL (default 1 day).
6. Assembly: vectors are read by offset within each chunk; results are returned with distances and basic metadata.

Minimal is on purpose. There are no knobs unless they’re necessary for the POC.

## Data layout: documents, chunks, offsets

Vectors are stored in S3 in grouped chunks. Every vector is addressable by a 64-bit pointer composed of three parts:

- document (27 bits)
- chunk (20 bits)
- offset (16 bits)

This gives the following limits (from `src/config/contants.py`):

- max documents: 2^27 = 134,217,728
- max chunks per document: 2^20 = 1,048,576
- max vectors per chunk: 2^16 = 65,536

Chunk ID is generated by combining document and chunk. The pointer encoding/decoding lives in `src/vectors/pointer.py` and provides:

- encode(document, chunk, offset) → index
- decode(index) → (document, chunk, offset)
- generate_chunk_id(document, chunk) → chunk_id

S3 keys are derived from chunk_id. A query pulls an entire chunk; individual vectors are taken by offset.

## Components (current POC)

- Index: `src/index/index.py` (HNSW over vector IDs)
- Cache: `src/cache/cache_layer.py` (Redis/FakeRedis, pickle serialization)
   - batch_get(keys) and batch_set(items, ttl) use Redis pipelines
   - default TTL for chunks: 24h
- Storage: `src/s3/chunk_upload.py` (upload/download of vector chunks), `src/s3/mock_client.py` for tests
- Query path: `src/query.py` (cache-first chunk resolution, S3 fallback), grouping helpers
- Pointer & limits: `src/vectors/pointer.py`, `src/config/contants.py`

## How to run

### Quick start (recommended)

```zsh
# from repo root
chmod +x setup.sh
./setup.sh
```

This creates/activates a venv, installs requirements, and launches the FastAPI server (uvicorn api.main:app) on http://localhost:8000.

### Manual setup

```zsh
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Run tests

```zsh
./.venv/bin/pytest -q
```

### Run the FastAPI server (manual)

```zsh
./.venv/bin/uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

Notes:
- The cache defaults to FakeRedis for simplicity. If you prefer a real Redis, run one (e.g., via Docker) and point `CacheLayer` to it.

## Proof-of-concept status

This project is intentionally small and minimal. It demonstrates the end-to-end idea without extra features. Expect rough edges and many opportunities to optimize or harden.

## Future improvements

### Optimization

- Parallelize S3 downloads for missing chunks (thread pool or async I/O)
- Smarter batching at query time (coalesce repeated chunk_ids across a batch of queries)
- Index persistence/loading and warm-start strategies
- Vector compression/quantization for S3 (smaller chunks, faster transfers)
- Observability: simple metrics for hit rate and latency
- Backpressure and timeouts for S3 on high load

### Cache strategies

- Configurable policies (TTL, LFU/LRU, pin hot chunks)
- Memory-mapped local cache option
- Support alternate backends (e.g., Valkey, in-process dict for tiny setups)

## Notes

- This is a POC: clarity over optimization. Keep it simple.
- The API surface is small on purpose; swap pieces if you need to.


